---
title: "twitteRStorm"
author: "Doug Raffle"
date: "6/13/2015"
output:
  ioslides_presentation:
    keep_md: yes
    widescreen: yes
subtitle: Prototyping a Streaming Framework for Analyzing Tweets with Storm
bibliography: twitteRStorm_biblio.bib
csl: bibtex.csl
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 5, fig.align = "center", 
                      message = FALSE, warning=FALSE, cache = TRUE, eval = TRUE) 
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tidyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
ps <- element_text(size = 12)
ps2 <- element_text(size = 15)
ps3 <- element_text(size = 20)
this.theme <- theme(axis.title.x = ps2, axis.title.y = ps2, axis.text.x = ps, axis.text.y = ps, 
                    title = ps3,
                    legend.text = ps)
```

## Topics

- Streaming Data
- The Storm Framework
- Storm Topologies
- Prototyping with `RStorm`
- Case Study: Prototyping a Twitter Tracker
- Bridging the Gap: the `Storm` Package and the Multi-Language Protocol
- Bridging the Gap: Twitter Streaming APIs

## Streaming Data
What makes streaming data a special case?

- Information is constantly flowing
- Analysis/Models need to update as new information comes in
- Deliverables are (often) time sensitive
- Data piles up quickly -- Can't store everything


## Meet the Neighbors
Hadoop and Spark:

- Distributed
- Fault Tolerant
- Scalable
- Designed to analyze static data in batches
- Run batch once, get results

Spark has an answer for streaming data:

- Spark Streaming
- Works on "micro-batches" of data

## What is Storm?

Storm [@storm]:

- Distributed
- Fault Tolerant
- Scalable
- Designed for streaming data
- Runs constantly, updates results as new data comes in


## Who uses Storm?
Many companies you (probably) use every day [@usedby]:

- Twitter
- The Weather Channel
- Spotify
- Yahoo!
- WebMD

What do they all have in common?

- New information constantly coming in
- Users who want speed and accuracy

## The Storm Framework
What makes up a storm framework?

The **Topology** specifies:

- Where data comes from
- How we process each datum
- Where results are stored

Once a topology is running:

- New data enters the topology as it arrives
- The topology extracts relevant information
- Each datum is processed
- Applications or databases are updated

## Storm Topologies
Storm frameworks are specified by *topologies* consisting of:

**Spouts:**

- Data sources, e.g., Twitter
- Every topology has at least one spout

**Bolts:**

- Process individual pieces of data
- Receive data from spouts or other bolts
- Send data to other bolts
- Store results in a database

## Storm: Data Structures
The basic data structure in Storm is a **tuple**.

- A key-value pair representing an observation (*a la* Hadoop)
- Spouts *emit* tuples as new data comes in
- Bolts *consume* tuples, process them, and emit 0 or more new tuples to other bolts

To aggregate results, bolts can also read from and write to more persistent data structures.

- Hash Maps -- quick, global storage, internal to topology
- Databases -- more persistent storage, accessible by outside applications

## The Topology Visualized
<img style="width: 1000px; height: 600px; float: center;" src="sample_topo.png">

## Getting Storm Running
So how do we get Storm up-and-running?

- Storm is a complex framework with many dependencies
- Usually requires a data engineering team to implement at scale
- Even a local set up is time consuming and requires a fair amount of technical knowledge

Is there a Vagrant box?

- Wirbelsturm [@wirbel] - Germal for "cyclone"
- Comes pre-loaded, but not as user-friendly as Tessera boxes are for Hadoop/Spark
- No step-by-step tutorials

## Developing Topologies
Once Spark is installed, what language do we use to create topologies?

Spouts

- Written in a Java Virtual Machine (JVM) languages, e.g. Java, Clojure, Scala

Bolts

- Each one is a separate source file
- Can be written in any language using the Multi-Language Protocol
- Non-JVM languages (e.g. R, Python) must be wrapped in a JVM function

Topology

- Specified in YAML, packaged by Maven

## `RStorm`
Most statisticians and data scientists aren't fluent in JVM languages.  What do we do?

The `RStorm` package [@rstorm] is designed to *simulate* a Storm topology.

- `R` programmers can develop a topology in a familiar language
- Organizations can evaluate whether or not Storm is appropriate for their project

## RStorm
`RStorm` is:

- A simulation of Storm
- A first draft or scratch pad

`RStorm` is **not**:

- An equivalent of `Rhipe`/`RHadoop`/`SparkR`
- A way of communicating with Storm through `R`
- Used to write bolts that Storm can read (yet)

## `RStorm` Overview
`RStorm` works by simulating topologies with `R` functions and `data.frames`

- A spout is simulated by a `data.frame`, observations are emitted sequentially by row
- A tuple is a one row `data.frame`
- Bolts are functions which take a single tuple (`data.frame`) as input
- Storage is done exclusively in a hash map, where each record is a key-value pair of the form (`name`, `data.frame`)
- Hashes can be read/written from inside the topology
- "Trackers" can only be written to row-by-row from within the topology

## Top-Level `RStorm` Functions
These functions are used to specify the topology, run it, and get the results:

Function | Purpose
---------|------------
`Topology(spout, ...)` | Creates a topology from a `data.frame`
`Bolt(FUNC, listen = 0, ...)` | Creates a bolt from a function, `listen` specifies the source of tuples
`AddBolt(topology, bolt, ...)` | Adds a bolt to the topology
`RStorm(topology, ...)` | Runs a fully-specified topology
`GetHash(name, results)` | Retrieves a hash from the finished topology
`GetTrack(name, results)` | Retrieves a track from a completed topology

## Within-Bolt Functions
These functions are called within bolts to perform operations:

Function | Purpose
---------|--------------
`Tuple(x, ...)` | Generates a tuple from a one-row `data.frame`
`GetHash(name, ...)` | Retreives a hash from within the topology
`SetHash(name, data)` | Saves a `data.frame` to the hash with key `name`
`TrackRow(name, data)`| Row binds a one-row `data.frame` to a tracked `data.frame`
`Emit(x, ...)` | Emits a tuple from within a bolt

## Example: Summing Values
As a small example, consider the following:

- Observations coming in are numbers
- The final result is the sum of the numbers
- We want to track the sum over time

We will write a small sample topology to illustrate the ideas of `RStorm`

## Example Topology
<img style="width: 900px; height: 600px; float: center;" src="example_topo.png">


## The Spout
```{r}
library(RStorm)
(dat <- data.frame(X = 1:5))
topology <- Topology(dat)
```

## Bolt 1: Current Sum
```{r}
get.sum <- function(tuple, ...){
  current.val <- tuple$X
  
  past.sum <- GetHash("current.sum")
  if(!is.data.frame(past.sum)) past.sum <- data.frame(Sum = 0)
  
  current.sum <- past.sum$Sum + current.val
  SetHash("current.sum", data.frame(Sum = current.sum))
  Emit(Tuple(data.frame(Sum = current.sum)), ...)
}
topology <- AddBolt(topology, Bolt(get.sum, listen = 0, boltID = 1))
```

## Bolt 2: Track Sum
```{r}
track.sum <- function(tuple, ...){
  current.sum <- tuple$Sum
  TrackRow("track.sum", data.frame(Sum = current.sum))
}
topology <- AddBolt(topology, Bolt(track.sum, listen = 1, boltID = 2))
topology
```

## Get the Results
```{r}
results <- RStorm(topology)
GetHash("current.sum", results)
t(GetTrack("track.sum", results))
```

## Case Study: Twitter
You're a data scientist working for Comcast, and management wants to monitor tweets mentioning your company.  In particular, they want to know:

1. What are people talking about?
2. Are they saying positive or negative things?
3. What are the common topics of the good and bad tweets?
4. Can we track the percentage of good and bad tweets over time?
5. Can you tell us when people start talking about us more?
6. Can they have a dashboard for the marketing team to monitor, like [this](http://raffled.shinyapps.io/comcast_dash)?


## Twitter: Which Platform?

**Small $n$, not time sensitive**

- Batch processing in `R` by day or week is probably good enough

**Small $n$, time sensitive**

- We can probably get away with an automated script and writing to a file/data base from within `R`

**Large $n$, not time sensitive**

- Hadoop or Spark batches run daily or weekly

**Large $n$, time sensitive**

- Storm (or Spark Streaming).  Assume this is the case.

## Prototyping the Stream
For our stream, we'll need:

- A `data.frame` of tweets for a spout

Bolts to:

- Track the rate of tweets using their time stamps
- Clean the text for sentiment analysis and word counts
- Count the words for the word cloud
- Classify the polarity
- Calculate the polarity over time
- Keep track of the words used in each polarity class

## Hashes
To store the tweets and track the information we need, we'll need hashes and trackers.

Hashes (read/write):

- Each tweet's time stamp
- Word Counts
- Polarity of each tweet
- Words associated with each polarity

Trackers (write row-by-row):

- Tweets per Minute (TPM) at each new tweet
- Polarity Percentages updated as tweets come in

```{r, echo = FALSE}
#### load packages for twitter, storm, etc
library(RStorm)
library(twitteR)
library(sentiment)
library(wordcloud)
library(dplyr)
library(tidyr)

################################ Get Tweets ################################
#### authorize API access
## consumer.key <- "********"
## consumer.secret <- "********"
## access.token <- "********"
## access.secret <- "********"
## setup_twitter_oauth(consumer.key, consumer.secret,
##                     access.token, access.secret)

## source("twitterAuth.R") ## stored credentials doesn't seem to work,
##                         ## hack around w/ authorization script

## comcast.list <- searchTwitter("comcast", n = 1500, lang = "en")
## comcast.df <- twListToDF(comcast.list)
## head(comcast.df)
## dim(comcast.df)
## ## Reverse order to emulate streaming data
## comcast.df <- comcast.df[order(comcast.df$created),]
## save(comcast.df, file = "comcastTweetsDF.bin")
load("../comcastTweetsDF.bin")  ## loads data.frame comcast.df

################################ Topology ################################
#### Saves some pain with working with strings in bols
options(stringsAsFactors = FALSE)

#### create the topology
topo <- Topology(comcast.df[1:200,], .verbose = FALSE) ## subset for testing

#### Grabs the current time stamp, looks at recent time stamps, and
#### calculates tweets per minute (tpm)
track.rate <- function(tuple, ...){
    t.stamp <- tuple$created
    ## track current time stamp
    t.stamp.df <- GetHash("t.stamp.df")
    if(!is.data.frame(t.stamp.df)) t.stamp.df <- data.frame()
    t.stamp.df <- rbind(t.stamp.df, data.frame(t.stamp = t.stamp))
    SetHash("t.stamp.df", t.stamp.df)
    
    ## get all time stamps find when a minute ago was
    t.stamp.past <- t.stamp.df$t.stamp
    last.min <- t.stamp - 60
    ## get tpm if we're a minute into the stream
    if(last.min >= min(t.stamp.past)){
        in.last.min <-  (t.stamp.past >= last.min) & (t.stamp.past <= t.stamp)
        tpm <- length(t.stamp.past[in.last.min])
    } else {
        tpm <- length(t.stamp.past)
    }
    TrackRow("tpm.df", data.frame(tpm = tpm, t.stamp = t.stamp))
}
topo <- AddBolt(topo, Bolt(track.rate, listen = 0, boltID = 1), .verbose = FALSE)

get.text <- function(tuple, ...){
    Emit(Tuple(data.frame(text = tuple$text,
                          t.stamp = tuple$created)), ...)
}
topo <- AddBolt(topo, Bolt(get.text, listen = 0, boltID = 2), .verbose = FALSE)

clean.text <- function(tuple, ...){
    text.clean <- tuple$text %>%
        ## convert to UTF-8
        iconv(to = "UTF-8") %>%
        ## strip URLs
        gsub("\\bhttps*://.+\\b", "", .) %>% 
        ## force lower case
        tolower %>%
        ## get rid of possessives
        gsub("'s\\b", "", .) %>%
        ## strip html special characters
        gsub("&.*;", "", .) %>%
        ## strip punctuation
        removePunctuation %>%
        ## make all whitespace into spaces
        gsub("[[:space:]]+", " ", .)
        
    names(text.clean) <- NULL ## needed to avoid RStorm missing name error?
    Emit(Tuple(data.frame(text = text.clean, t.stamp = tuple$t.stamp)), ...)
}
topo <- AddBolt(topo, Bolt(clean.text, listen = 2, boltID = 3), .verbose = FALSE)

strip.stopwords <- function(tuple, ...){
    text.content <- removeWords(tuple$text,
                                removePunctuation(stopwords("SMART"))) %>%
        gsub("[[:space:]]+", " ", .)
    if(text.content != " " && !is.na(text.content)){
        Emit(Tuple(data.frame(text = text.content,
                              t.stamp = tuple$t.stamp)), ...)
    }
}
topo <- AddBolt(topo, Bolt(strip.stopwords, listen = 3, boltID = 4), .verbose = FALSE)

get.word.counts <- function(tuple, ...){
    words <- unlist(strsplit(tuple$text, " "))
    words.df <- GetHash("word.counts.df")
    if(!is.data.frame(words.df)) words.df <- data.frame()
    sapply(words, function(word){
               if(word %in% words.df$word){
                   words.df[word == words.df$word,]$count <<-
                       words.df[word == words.df$word,]$count + 1
               } else{
                   words.df <<- rbind(words.df,
                                     data.frame(word = word, count = 1))
               }
           }, USE.NAMES = FALSE)
    SetHash("word.counts.df", words.df)
}
topo <- AddBolt(topo, Bolt(get.word.counts, listen = 4, boltID = 5), .verbose = FALSE)

get.polarity <- function(tuple, ...){
    polarity <- classify_polarity(tuple$text)[,4]
    Emit(Tuple(data.frame(text = tuple$text,
                          t.stamp = tuple$t.stamp,
                          polarity = polarity)), ...)
}
topo <- AddBolt(topo, Bolt(get.polarity, listen = 4, boltID = 6), .verbose = FALSE)

track.polarity <- function(tuple, ...){
    polarity.df <- GetHash("polarity.df")
    if(!is.data.frame(polarity.df)) polarity.df <- data.frame()
    polarity.df <- rbind(polarity.df,
                         data.frame(polarity = tuple$polarity))
    SetHash("polarity.df", polarity.df)
    polarity <- polarity.df$polarity

    polar.mat <- cbind(p.positive = (polarity == "positive"),
                       p.neutral = (polarity == "neutral"),
                       p.negative = (polarity == "negative"))
    prop.df <- data.frame(t(colMeans(polar.mat, na.rm = TRUE)),
                          t.stamp = tuple$t.stamp)
    TrackRow("prop.df", prop.df)
}
topo <- AddBolt(topo, Bolt(track.polarity, listen = 6, boltID = 7), .verbose = FALSE)

store.words.polarity <- function(tuple, ...){
    polar.words.df <- GetHash("polar.words.df")
    if(!is.data.frame(polar.words.df)) polar.words.df <- data.frame()
    
    words <- unlist(strsplit(tuple$text, " "))
    polarity <- tuple$polarity

    if(polarity %in% rownames(polar.words.df)){
        polar.words.df[polarity,][[1]] <- list(append(polar.words.df[polarity,][[1]], words))
    } else{
        word.list <- list(words)
        names(word.list) <- eval(polarity)
        polar.words.df <- rbind(polar.words.df, as.matrix(word.list))
    }
    SetHash("polar.words.df", polar.words.df)
}
topo <- AddBolt(topo, Bolt(store.words.polarity, listen = 6, boltID = 8), .verbose = FALSE)

#### get results
result <- RStorm(topo)
options(stringsAsFactors = TRUE)
```

## Implementing the Twitter Stream
Implementing the stream is the topic of the tutorial, for now we will:

- Describe the topology
- Describe the bolts needed
- Describe the hashes and trackers

## Getting Tweets
Since `RStorm` needs a `data.frame` as input, how do we get tweets?

- `R` package `twitteR` can access Twitter's REST APIs [@rest] 
- REST APIs search recent tweets by keyword, location, timeframe, etc.
- Only has access to a few days worth of tweets
- Tweets for this example were pulled May 27, right after Time Warner Cable and Charter announced their merger
- We will discuss the REST APIs in more detail during the tutorial


## The Bolts
Bolt | Purpose
-----|-------------
`track.rate()` | Calculate and track tweets per minute over time
`get.text()` | Extract text from tweet
`clean.text()` | Clean special characters, links, punctuation, etc.
`strip.stopwords()` | Clean conjunctions, prepositions, etc.
`get.word.counts()` | Create and update word counts
`get.polarity()` | Classify polarity of a tweet
`track.polarity()` | Track percentage of positive/negative/neutral tweets over time
`store.words.polarity()` | Store words for each polarity level

## Data Frames

`data.frame` | Role | Description
-------------|------|---------------
`comcast.df` | Spout | Table to simulate tweets 
`word.counts.df` | Hash | Stores word frequencies
`t.stamp.df` | Hash | Store unique time stamps
`tpm.df` | Tracker | Track tweets per minute over time
`prop.df` | Tracker | Track percentage per polarity over time
`polarity.df` | Hash | Store polarity per tweet
`polar.words.df` | Hash | Keep track of words associated with each polarity

## The Topology
<img style="width: 1000px; height: 600px; float: center;" src="my_topology.png">

## Running the Topology
```{r}
topo
```
```{r, eval = FALSE}
result <- RStorm(topo)
```

## Analyzing the Results
To analyze the results we need to:

- Read `word.counts.df` and make a wordcloud
- Read `polar.words.df` and make a comparison cloud
- Read `tpm.df` and make a timeplot
- Read `prop.df` and make a timeplot

We'll do this in the tutorial.


## Bridging the Gap
How do we go past prototyping with `RStorm` to developing for Storm in `R`?

We may need to:

- Write bolts in `R`.
- Access new tweets as they come, not read them from the past

## Bridging the Gap: The MLP
How do we write Storm bolts in `R` (or any other language)?

- Tuples are passed to non-JVM languages using Storm's Multi-Language Protocol (MLP)
- The MLP is just a JSON-like text format for storing tuples
- Tuples are passed using standard input/output (*a la* Hadoop)



## The MLP
Say we wanted to write a simple bolt which took a sentence and split it into individual words.  What would the sentence tuple look like in the MLP?

```
{
  // The tuple's id - this is a string to support languages lacking 64-bit precision
  "id": "-6955786537413359385",
  // The id of the component that created this tuple
  "comp": "1",
  // The id of the stream this tuple was emitted to
  "stream": "1",
   // The id of the task that created this tuple
  "task": 9,
  // All the values in this tuple
  "tuple": [1, "snow white and the seven dwarfs"]
}
end
```

## Bridging the Gap: The `Storm` Package

The `R` package `Storm` [@stormr]:

- Can read tuples in MLP from STDIN
- Emits new tuples in MLP to STDOUT
- Is implemented using reference classes

How do we use it?

- Create a `Storm` object
- Bolt functions are anonymous (lambda) functions in the `Storm` object
- The object will listen for tuples and call the lambda function for each tuple
- Also supports utility functions, like `ack`, `log`, `fail`, etc.

## `Storm` Bolt
```{r eval = FALSE}
## bolt.R
## Load Storm Library
library(Storm)
## create the Storm object
storm <- Storm$new()
## create bolt function
storm$lambda <- function(s){
    tuple <- s$tuple
    words <- unlist(strsplit(as.character(tuple$input[2]), " "))
    sapply(words, function(word){
               tuple$output <- vector("character", 1)
               tuple$output[1] <- word
               s$emit(tuple)
           })
}
storm$run()
```

## Using a `Storm` Bolt
In Storm, a bolt written in a JVM language would call your `R` script containing the bolt code.  

Because Storm uses standard I/O for the MLP, you can simulate in a shell.
```shell
$ cat tuple.txt | Rscript bolt.R
{"command": "emit", "anchors": ["-6955786537413359385"], "tuple": "snow"}
end
{"command": "emit", "anchors": ["-6955786537413359385"], "tuple": "white"}
end
{"command": "emit", "anchors": ["-6955786537413359385"], "tuple": "and"}
end
{"command": "emit", "anchors": ["-6955786537413359385"], "tuple": "the"}
end
{"command": "emit", "anchors": ["-6955786537413359385"], "tuple": "seven"}
end
{"command": "emit", "anchors": ["-6955786537413359385"], "tuple": "dwarfs"}
end
```

## Bridging the Gap: Live Tweets
For `RStorm`, we used the REST APIs to grab recent tweets.

In a live Storm environment, we'll need to process tweets as they come.

- Twitter's Streaming APIs [@streaming] offer an "always on" connection to Twitter
- We can filter and search by keyword, location, etc. just like with the REST APIs
- Storm can use Twitter as a spout using these APIs


## Tutorial
Source files on Github: 

[https://github.com/raffled/twitteRStorm/](https://github.com/raffled/twitteRStorm/)


Tutorial Source Code: 

[stat.wvu.edu/~draffle/twitteRStorm/tutorial/tutorial.Rmd](http://stat.wvu.edu/~draffle/twitteRStorm/tutorial/tutorial.Rmd)


Rendered HTML:

[stat.wvu.edu/~draffle/tutorial/tutorial.html](http://stat.wvu.edu/~draffle/twitteRStorm/tutorial/tutorial.html)



## References





















