---
title: "twitteRStorm"
author: "Doug Raffle"
date: "6/13/2015"
output:
  ioslides_presentation:
    keep_md: yes
    widescreen: yes
subtitle: Prototyping a Streaming Framework for Analyzing Tweets with Storm
bibliography: twitteRStorm_biblio.bib
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 5, fig.align = "center", 
                      message = FALSE, warning=FALSE, cache = TRUE, eval = TRUE) 
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tidyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
ps <- element_text(size = 12)
ps2 <- element_text(size = 15)
ps3 <- element_text(size = 20)
this.theme <- theme(axis.title.x = ps2, axis.title.y = ps2, axis.text.x = ps, axis.text.y = ps, 
                    title = ps3,
                    legend.text = ps)
```

## Topics

- Streaming Data
- The Storm Framework
- Storm Topologies
- Prototyping with `RStorm`
- Case Study: Prototyping a Twitter Tracker
- Bridging the Gap: the `Storm` Package and the Multi-Language Protocol

## Streaming Data
What makes streaming data a special case?

- Information is constantly flowing
- Analysis/Models need to be able to update as new information comes in
- Deliverables are (often) time sensitive
- Data piles up quickly -- Can't store everything


## Meet the Neighbors
Hadoop and Spark:

- Distributed
- Fault Tolerant
- Scalable
- Designed to analyze static data in batches
- Run batch once, get results

Spark has an answer for streaming data:

- Spark Streaming
- Works on "micro-batches" of data

## What is Storm?

Storm [@storm]:

- Distributed
- Fault Tolerant
- Scalable
- Designed for streaming data
- Runs constantly, updates results as new data comes in


## Who uses Storm?
Many companies you (probably) use every day [@usedby]:

- Twitter
- The Weather Channel
- Spotify
- Yahoo!
- WebMD

What do they all have in common?

- New information constantly coming in
- Users who want speed and accuracy

## The Storm Framework
What makes up a storm framework?

The **Topology** specifies:

- Where data comes from
- What gets down to each piece of data
- Where results are stored

Once a topology is running:

- New data enters the topology as it arrives
- The topology extracts relevant information
- Applications or databases are updated

## Storm Topologies
Storm frameworks are specified by "topologies" consisting of spouts and bolts.

**Spouts:**

- Data sources, e.g., Twitter
- Every topology has at least one spout

**Bolts:**

- Process individual pieces of data
- Receive data from spouts or other bolts

## Storm: Data Structures
The basic data structure in Storm is a **tuple**.

- A key-value pair representing an observation
- Spouts *emit* tuples as new data comes in
- Bolts *consume* tuples, process them, and emit 0 or more new tuples to other bolts

To aggregate results, bolts can also read from and write to more persistent data structures.

- Hash Maps -- quick, global storage internal to topology
- Databases -- more persistent storage for outside applications

## The Topology Visualized
<img style="width: 1000px; height: 600px; float: center;" src="sample_topo.png">

## Getting Storm Running
So how do we get Storm up-and-running?

- Storm is a complex framework with many dependencies
- Requires a data engineering team to implement at scale
- Even a local set up is time consuming and requires a fair amount of technical knowledge

Is there a Vagrant box?

- Wirbelsturm [@wirbel] - Germal for "cyclone"
- Comes pre-loaded, but not as user-friendly as Tessera boxes are for Hadoop/Spark
- No step-by-step tutorials

## Developing Topologies
What language do we use to create topologies?  It depends.

Spouts

- Written in a Java Virtual Machine (JVM) languages, e.g. Java, Clojure, Scala

Bolts

- Each one is a separate source file
- Can be written in any language using the Multi-Language Protocol
- Non-JVM languages (e.g. R, Python) must be wrapped in a JVM function

Topology

- Specified in YAML, packaged by Maven

## RStorm
Most statisticians and data scientists aren't fluent in JVM languages.  What do we do?

The `RStorm` package [@rstorm] is designed to *simulate* a Storm topology.

- `R` programmers can develop a topology in a familiar language
- Organizations can evaluate whether or not Storm is appropriate for their project

## RStorm
RStorm is:

- A simulation of Storm
- A first draft or scratch pad

RStorm is **not**:

- An equivalent of `Rhipe`/`RHadoop`/`SparkR`
- A way of communicating with Storm through `R`
- Used to write bolts that Storm can read (though this is an eventual goal)

## `RStorm` Overview
`RStorm` works by simulating topologies with `R` functions and data frames

- A spout is simulated by a `data.frame`, observations are emitted sequentially by row
- A tuple is a one row `data.frame`
- Bolts are functions which take a single tuple (`data.frame`) as input
- Storage is done exclusively in a hash map, where each record is a key-value pair where the key is a character string and the value is a `data.frame`
- Hashes can be read/write from inside the topology, or "trackers," which can only be written to row-by-row from within the topology

## Top-Level `RStorm` Functions
These functions are used to specify the topology, run it, and get the results:

Function | Purpose
---------|------------
`Topology(spout, ...)` | Creates a topology from a `data.frame`
`Bolt(FUNC, listen = 0, ...)` | Creates a bolt from a function, `listen` specifies the source of tuples
`AddBolt(topology, bolt, ...)` | Adds a bolt to the topology
`RStorm(topology, ...)` | Runs a fully-specified topology
`GetHash(name, topology)` | Retrieves a hash from the finished topology
`GetTrack(name, topology)` | Retrieves a track from a completed topology

## Within-Bolt Functions
These functions are called within bolts to perform operations:

Function | Purpose
---------|--------------
`Tuple(x, ...)` | Generates a tuple from a one-row `data.frame`
`GetHash(name, ...)` | Retreives a hash from within the topology
`SetHash(name, data)` | Saves a `data.frame` to the hash with key `name`
`TrackRow(name, data)`| Row binds a one-row `data.frame` to a tracked `data.frame`
`Emit(x, ...)` | Emits a tuple from within a bolt

## Example: Summing Values
As a small example, consider the following:

- Observations coming in are numbers
- The final result is the sum of the numbers
- We want to track the sum over time

We will write a small sample topology to illustrate the ideas of `RStorm`

## Example Topology
<img style="width: 900px; height: 600px; float: center;" src="example_topo.png">


## The Spout
```{r}
library(RStorm)
(dat <- data.frame(X = 1:5))
topology <- Topology(dat)
```

## Bolt 1: Current Sum
```{r}
get.sum <- function(tuple, ...){
  current.val <- tuple$X
  
  past.sum <- GetHash("current.sum")
  if(!is.data.frame(past.sum)) past.sum <- data.frame(c.sum = 0)
  
  current.sum <- past.sum$c.sum + current.val
  SetHash("current.sum", data.frame(c.sum = current.sum))
  Emit(Tuple(data.frame(c.sum = current.sum)), ...)
}
topology <- AddBolt(topology, Bolt(get.sum, listen = 0, boltID = 1))
```

## Bolt 2: Track Sum
```{r}
track.sum <- function(tuple, ...){
  current.sum <- tuple$c.sum
  TrackRow("track.sum", data.frame(c.sum = current.sum))
}
topology <- AddBolt(topology, Bolt(track.sum, listen = 1, boltID = 2))
topology
```

## Get the Results
```{r}
results <- RStorm(topology)
GetHash("current.sum", results)
t(GetTrack("track.sum", results))
```

## Case Study: Twitter
You're a data scientist working for Comcast, and management wants to monitor tweets mentioning your company.  In particular, they want to know:

1. What are people talking about?
2. Are they saying positive or negative things?
3. What are the common topics of the good and bad tweets?
4. Can we track the percentage of good and bad tweets over time?
5. Can you tell us when people start talking about us more?
6. Can they have a dashboard for the marketing team to monitor, like [this](http://raffled.shinyapps.io/comcast_dash)?

## Twitter: Translating to Stats
First, we need to translate the requests to techniques and visualizations.

What are people talking about?

- Word frequencies $\to$ wordclouds

Are they saying positive or negative things?

- Classify the polarity (sentiment analysis)

What are common topics of good and bad tweets?

- Word frequencies by polarity $\to$ comparison clouds

## Twitter: Translating to Stats

Can we track the percentage of good and bad tweets over time?

- Track percent of each polarity classification over time $\to$ timeplots

Can you tell us when people start talking about us more?

- Track rate of tweets $\to$ timeplot

Can they get a dashboard?

- `shinydash`, Tableau, or Javascript

## Twitter: Which Platform?

**Small $n$, not time sensitive**

- Batch processing in `R` by day or week is probably good enough

**Small $n$, time sensitive**

- We can probably get away with an automated script and writing to a file/data base from within `R`

**Large $n$, not time sensitive**

- Hadoop or Spark batches run daily or weekly

**Large $n$, time sensitive**

- Storm (or Spark Streaming).  Assume this is the case.

## Prototyping the Stream
For our stream, we'll need:

- A `data.frame` of tweets for a spout

Bolts to:

- Track the rate of tweets using their time stamps
- Clean the text for sentiment analysis and word counts
- Count the words for the word cloud
- Classify the polarity
- Calculate the polarity over time
- Keep track of the words used in each polarity class

## Hashes
To store the tweets and track the information we need, we'll need hashes and trackers.

Hashes (read/write):

- Word Counts
- Polarity of each tweet

Trackers (write row-by-row):

- Tweets per Minute (TPM) at each new tweet
- Polarity Percentages updated as tweets come in

```{r, echo = FALSE}
#### load packages for twitter, storm, etc
library(RStorm)
library(twitteR)
library(sentiment)
library(wordcloud)
library(dplyr)
library(tidyr)

################################ Get Tweets ################################
#### authorize API access
## consumer.key <- "********"
## consumer.secret <- "********"
## access.token <- "********"
## access.secret <- "********"
## setup_twitter_oauth(consumer.key, consumer.secret,
##                     access.token, access.secret)

## source("twitterAuth.R") ## stored credentials doesn't seem to work,
##                         ## hack around w/ authorization script

## comcast.list <- searchTwitter("comcast", n = 1500, lang = "en")
## comcast.df <- twListToDF(comcast.list)
## head(comcast.df)
## dim(comcast.df)
## ## Reverse order to emulate streaming data
## comcast.df <- comcast.df[order(comcast.df$created),]
## save(comcast.df, file = "comcastTweetsDF.bin")
load("../comcastTweetsDF.bin")  ## loads data.frame comcast.df

################################ Topology ################################
#### Saves some pain with working with strings in bols
options(stringsAsFactors = FALSE)

#### create the topology
topo <- Topology(comcast.df[1:200,], .verbose = FALSE) ## subset for testing

#### Grabs the current time stamp, looks at recent time stamps, and
#### calculates tweets per minute (tpm)
track.rate <- function(tuple, ...){
    t.stamp <- tuple$created
    ## track current time stamp
    t.stamp.df <- GetHash("t.stamp.df")
    if(!is.data.frame(t.stamp.df)) t.stamp.df <- data.frame()
    t.stamp.df <- rbind(t.stamp.df, data.frame(t.stamp = t.stamp))
    SetHash("t.stamp.df", t.stamp.df)
    
    ## get all time stamps find when a minute ago was
    t.stamp.past <- t.stamp.df$t.stamp
    last.min <- t.stamp - 60
    ## get tpm if we're a minute into the stream
    if(last.min >= min(t.stamp.past)){
        in.last.min <-  (t.stamp.past >= last.min) & (t.stamp.past <= t.stamp)
        tpm <- length(t.stamp.past[in.last.min])
    } else {
        tpm <- length(t.stamp.past)
    }
    TrackRow("tpm.df", data.frame(tpm = tpm, t.stamp = t.stamp))
}
topo <- AddBolt(topo, Bolt(track.rate, listen = 0, boltID = 1), .verbose = FALSE)

get.text <- function(tuple, ...){
    Emit(Tuple(data.frame(text = tuple$text,
                          t.stamp = tuple$created)), ...)
}
topo <- AddBolt(topo, Bolt(get.text, listen = 0, boltID = 2), .verbose = FALSE)

clean.text <- function(tuple, ...){
    text.clean <- tuple$text %>%
        ## convert to UTF-8
        iconv(to = "UTF-8") %>%
        ## strip URLs
        gsub("\\bhttps*://.+\\b", "", .) %>% 
        ## force lower case
        tolower %>%
        ## get rid of possessives
        gsub("'s\\b", "", .) %>%
        ## strip html special characters
        gsub("&.*;", "", .) %>%
        ## strip punctuation
        removePunctuation %>%
        ## make all whitespace into spaces
        gsub("[[:space:]]+", " ", .)
        
    names(text.clean) <- NULL ## needed to avoid RStorm missing name error?
    Emit(Tuple(data.frame(text = text.clean, t.stamp = tuple$t.stamp)), ...)
}
topo <- AddBolt(topo, Bolt(clean.text, listen = 2, boltID = 3), .verbose = FALSE)

strip.stopwords <- function(tuple, ...){
    text.content <- removeWords(tuple$text,
                                removePunctuation(stopwords("SMART"))) %>%
        gsub("[[:space:]]+", " ", .)
    if(text.content != " " && !is.na(text.content)){
        Emit(Tuple(data.frame(text = text.content,
                              t.stamp = tuple$t.stamp)), ...)
    }
}
topo <- AddBolt(topo, Bolt(strip.stopwords, listen = 3, boltID = 4), .verbose = FALSE)

get.word.counts <- function(tuple, ...){
    words <- unlist(strsplit(tuple$text, " "))
    words.df <- GetHash("word.counts.df")
    if(!is.data.frame(words.df)) words.df <- data.frame()
    sapply(words, function(word){
               if(word %in% words.df$word){
                   words.df[word == words.df$word,]$count <<-
                       words.df[word == words.df$word,]$count + 1
               } else{
                   words.df <<- rbind(words.df,
                                     data.frame(word = word, count = 1))
               }
           }, USE.NAMES = FALSE)
    SetHash("word.counts.df", words.df)
}
topo <- AddBolt(topo, Bolt(get.word.counts, listen = 4, boltID = 5), .verbose = FALSE)

get.polarity <- function(tuple, ...){
    polarity <- classify_polarity(tuple$text)[,4]
    Emit(Tuple(data.frame(text = tuple$text,
                          t.stamp = tuple$t.stamp,
                          polarity = polarity)), ...)
}
topo <- AddBolt(topo, Bolt(get.polarity, listen = 4, boltID = 6), .verbose = FALSE)

track.polarity <- function(tuple, ...){
    polarity.df <- GetHash("polarity.df")
    if(!is.data.frame(polarity.df)) polarity.df <- data.frame()
    polarity.df <- rbind(polarity.df,
                         data.frame(polarity = tuple$polarity))
    SetHash("polarity.df", polarity.df)
    polarity <- polarity.df$polarity

    polar.mat <- cbind(p.positive = (polarity == "positive"),
                       p.neutral = (polarity == "neutral"),
                       p.negative = (polarity == "negative"))
    prop.df <- data.frame(t(colMeans(polar.mat, na.rm = TRUE)),
                          t.stamp = tuple$t.stamp)
    TrackRow("prop.df", prop.df)
}
topo <- AddBolt(topo, Bolt(track.polarity, listen = 6, boltID = 7), .verbose = FALSE)

store.words.polarity <- function(tuple, ...){
    polar.words.df <- GetHash("polar.words.df")
    if(!is.data.frame(polar.words.df)) polar.words.df <- data.frame()
    
    words <- unlist(strsplit(tuple$text, " "))
    polarity <- tuple$polarity

    if(polarity %in% rownames(polar.words.df)){
        polar.words.df[polarity,][[1]] <- list(append(polar.words.df[polarity,][[1]], words))
    } else{
        word.list <- list(words)
        names(word.list) <- eval(polarity)
        polar.words.df <- rbind(polar.words.df, as.matrix(word.list))
    }
    SetHash("polar.words.df", polar.words.df)
}
topo <- AddBolt(topo, Bolt(store.words.polarity, listen = 6, boltID = 8), .verbose = FALSE)

#### get results
result <- RStorm(topo)
```

## Implementing the Twitter Stream
Implementing the stream is the topic of the tutorial, for now we will:

- Describe the topology
- Describe the bolts needed
- Describe the hashes and trackers
- Use the results

## The Topology
<img style="width: 1000px; height: 550px; float: center;" src="my_topology.png">

## The Bolts
Bolt | Purpose
-----|-------------
`track.rate()` | Calculate and track tweets per minute
`get.text()` | Extract text from tweet
`clean.text()` | Clean special characters, links, punctuation, etc.
`strip.stopwords()` | Clean conjunctions, prepositions, etc.
`get.word.counts()` | Create and update word counts
`get.polarity()` | Classify polarity of a tweet
`track.polarity()` | Track percentage of positive/negative/neutral tweets over time
`store.words.polarity()` | Store words for each polarity level



## References





















